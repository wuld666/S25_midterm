{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6.1 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import necessary libraries and specify that graphs should be plotted inline. \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting and Data\n",
    "We will use a standard dataset that predicts breast cancer for today's practice. Similar to the iris dataset, the breast_cancer dataset comes with Scikit-Learn package. This is a binary classification dataset (i.e., Y=1 if malignant, Y=0 if benign). We can use the same syntax to learn about the breast_cancer dataset (e.g., variables, observations, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "print(\"The dataset has\", cancer.data.shape[0], \"observations, and\", cancer.data.shape[1], \"variables\") \n",
    "print(\"We have\", cancer.target.shape[0], \"records for target\")\n",
    "print(\"The variables are\", list(cancer.feature_names))\n",
    "print(\"The targets are\", list(cancer.target_names))\n",
    "# Note: to map the y value with the y label(i.e., target_names):\n",
    "### => The index of each y label in the target_names is the y value\n",
    "### => for cancer dataset, malignant = 0, and benign = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test, if test size is not specified, default value  of 0.25 is applied\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, \n",
    "                                                    cancer.target, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a decision tree model, use syntax:\n",
    "\n",
    "**<center>sklearn.tree.DecisionTreeClassifier()</center>**\n",
    "\n",
    "**Key Inputs and Methods:**\n",
    "- criterion: the function to measure the quality of a split. Manually set to \"entropy\" for ID3-like algorithm.\n",
    "- Set random_state for replication purposes (technical). [in case of a tie]\n",
    "- Use .fit to train the model, .score to obtain the performance measure (accuracy), and .predict for prediction.\n",
    "\n",
    "**Other Inputs (when not growing the full tree):**\n",
    "- max_depth: depth of the tree. depth = 1 if split once.\n",
    "- max_leaf_nodes: the maximum number of leaves in a tree.\n",
    "- min_samples_leaf: minimum sample size in a leave node. \n",
    "- min_samples_split: the minimum sample size for a subset to be further split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attributes**\n",
    "- max_features: how many features are used to create a tree.\n",
    "- feature_importances_: the importance of each feature (based on entropy reduction). This may not be reliable if variables have high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice:**\n",
    "\n",
    "Train a decision tree classifier. Set random state to 0. Use entropy as the impurity measure.\n",
    "- Report the train and test accuracy.\n",
    "- Report the feature importance\n",
    "- Based on the feature importance, create a bar plot. (Use syntax: plt.bar(x, y). x is the label of bars, y is the height of bars. You may want to change \"bar\" to \"barh\" for better representation).\n",
    "- Create a bar plot of feature importance. Include only those with importance > 0 (Hint: logical indexing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt =  DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "dt.fit(X_train, y_train)\n",
    "dt.score(X_test, y_test), dt.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_x = cancer.feature_names\n",
    "my_y = dt.feature_importances_\n",
    "\n",
    "plt.barh(my_x, my_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_id = dt.feature_importances_ > 0\n",
    "\n",
    "my_x = cancer.feature_names[my_id]\n",
    "my_y = dt.feature_importances_[my_id]\n",
    "\n",
    "plt.barh(my_x, my_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Overfitting \n",
    "\n",
    "We twist the \"additional inputs\" from the Decision Tree to address the overfitting issue. Setting proper values for these parameters can be hard. We use GridSearchCV to search for the best combination of these parameter values.\n",
    "\n",
    "\n",
    "**Practice**:\n",
    "\n",
    "- Train a decision tree with maximum depth = 3. Then report the test accuracy.\n",
    "- Train a decision tree with GridSearchCV, report the best model's hyperparameters, as well as the test accuracy [The model can take several seconds to run]. Set random state to 0 for replication purpose.\n",
    "    - Choose max_depth from all positive integers below 10\n",
    "    - Choose min_samples_split from all integers between 2 and 10 (inclusive)\n",
    "    - Choose max_leaf_nodes from be integers between 2 and 10 (inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let maximum depth be 3\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# A Basic Tree\n",
    "dt_3 = DecisionTreeClassifier(random_state=0, max_depth=3, criterion='entropy')\n",
    "dt_3.fit(X_train, y_train)\n",
    "dt_3.score(X_test, y_test), dt_3.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch + CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "opt_tree = DecisionTreeClassifier(random_state = 0, criterion='entropy')\n",
    "\n",
    "dt_params = {'max_depth':  range(1,10)         ,\n",
    "             'min_samples_split':   range(2,11),\n",
    "             'max_leaf_nodes':    range(2,11)   }\n",
    "\n",
    "grid_tree = GridSearchCV(opt_tree, dt_params, n_jobs = 4)\n",
    "grid_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the best hyperparameters chosen\n",
    "\n",
    "grid_tree.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Visualizing the tree (Extension)*\n",
    "\n",
    "Use syntax: **tree.export_text(TREE_MODEL_NAME)** to obtain the rules. You need to import the tree module first. (i.e., from sklearn import tree)\n",
    "\n",
    "\n",
    "Use syntax: **sklearn.tree.plot_tree(estimator, feature_names, class_names, impurity)** to show the tree. \n",
    "\n",
    "Refer to the sample code below. *[plt.figure() is used create a figure object (you can imagine it as a canvas).]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "print(tree.export_text(grid_tree.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree\n",
    "from sklearn import tree\n",
    "fig = plt.figure(figsize=(20,10)) # set a proper figure size (in case that the figure is too small to read or ratio is not proper)\n",
    "\n",
    "tree.plot_tree(grid_tree.best_estimator_, \n",
    "               feature_names = cancer.feature_names, # specify variable names \n",
    "               class_names = cancer.target_names, # specify class (Y) names\n",
    "               filled = True, impurity = False) # whether to color the boxes, whether to report gini index\n",
    "             #   fontsize = 12) # set fontsize to read\n",
    "plt.show()\n",
    "\n",
    "# save the figure to read through the boxes, it is saved under the same directory as the coding doc.\n",
    "# fig.savefig(\"grid_tree.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
